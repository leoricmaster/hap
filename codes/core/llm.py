import os
from openai import OpenAI
from openai.types.chat import ChatCompletionMessageParam
from dotenv import load_dotenv
from typing import List, Optional, Iterator
from .exceptions import HelloAgentsException

load_dotenv()

class HelloAgentsLLM:
    def __init__(
            self,
            api_key: Optional[str] = None,
            base_url: Optional[str] = None,
            model: Optional[str] = None,
            temperature: float = 0.7,
            max_tokens: Optional[int] = None,
            timeout: Optional[int] = None
        ):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½ã€‚

        Args:
            
            api_key: APIå¯†é’¥ï¼Œå¦‚æœªæä¾›åˆ™ä» LLM_API_KEY è¯»å–
            base_url: æœåŠ¡åœ°å€ï¼Œå¦‚æœªæä¾›åˆ™ä» LLM_BASE_URL è¯»å–
            model: æ¨¡å‹åç§°ï¼Œå¦‚æœªæä¾›åˆ™ä» LLM_MODEL_ID è¯»å–
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼Œå¦‚æœªæä¾›åˆ™ä» LLM_TIMEOUT è¯»å–ï¼Œé»˜è®¤60ç§’
        """
        api_key = api_key or os.getenv("LLM_API_KEY")
        base_url = base_url or os.getenv("LLM_BASE_URL")
        self._model = model or os.getenv("LLM_MODEL_ID")
        self._temperature = temperature
        self._max_tokens = max_tokens
        timeout = timeout or int(os.getenv("LLM_TIMEOUT", 60))
        
        if not all([api_key, base_url, self._model]):
            raise ValueError("`base_url`, `api_key`, `model` must be provided or defined in .env file.")

        self._client = OpenAI(api_key=api_key, base_url=base_url, timeout=timeout)

    def think(self,messages: List[ChatCompletionMessageParam], **kwargs) -> Iterator[str]:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›æµå¼å“åº”ã€‚
        è¿™æ˜¯ä¸»è¦çš„è°ƒç”¨æ–¹æ³•ï¼Œé»˜è®¤ä½¿ç”¨æµå¼å“åº”ä»¥è·å¾—æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„å€¼

        Yields:
            str: æµå¼å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        assert self._model is not None
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self._model} æ¨¡å‹ ...")
        try:
            response = self._client.chat.completions.create(
                messages = messages,
                model = self._model,
                temperature = kwargs.get('temperature', self._temperature),
                max_tokens = kwargs.get('max_tokens', self._max_tokens),
                stream=True,
            )
            
            # å¤„ç†æµå¼å“åº”
            print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                if content:
                    print(content, end="", flush=True)
                    yield content
            print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ

        except Exception as e:
            print(f"âŒ è°ƒç”¨ LLM API æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise HelloAgentsException(f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")

    def invoke(self, messages: List[ChatCompletionMessageParam], **kwargs) -> str:
        """
        éæµå¼è°ƒç”¨LLMï¼Œè¿”å›å®Œæ•´å“åº”ã€‚
        é€‚ç”¨äºä¸éœ€è¦æµå¼è¾“å‡ºçš„åœºæ™¯ã€‚
        """
        try:
            assert self._model is not None
            response = self._client.chat.completions.create(
                messages=messages,
                model=self._model,
                temperature=kwargs.get('temperature', self._temperature),
                max_tokens=kwargs.get('max_tokens', self._max_tokens),
                **{k: v for k, v in kwargs.items() if k not in ['temperature', 'max_tokens']}
            )
            return response.choices[0].message.content
        except Exception as e:
            raise HelloAgentsException(f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")

    def stream_invoke(self, messages: List[ChatCompletionMessageParam], **kwargs) -> Iterator[str]:
        """
        æµå¼è°ƒç”¨LLMçš„åˆ«åæ–¹æ³•ï¼Œä¸thinkæ–¹æ³•åŠŸèƒ½ç›¸åŒã€‚
        ä¿æŒå‘åå…¼å®¹æ€§ã€‚
        """
        yield from self.think(messages, **kwargs)


if __name__ == '__main__':
    try:
        llmClient = HelloAgentsLLM()

        exampleMessages: List[ChatCompletionMessageParam] = [
            {"role": "system", "content": "You are a helpful assistant that writes Python code."},
            {"role": "user", "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•"}
        ]
        
        print("--- è°ƒç”¨LLM ---")
        responseText = llmClient.think(exampleMessages)

    except ValueError as e:
        print(e)